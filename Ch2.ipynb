{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 - Multi-armed Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1 ###\n",
    "**Q:** \n",
    "> In $\\epsilon$-greedy action selection, for the case of two actions and $\\epsilon=0.5$, what is the probability that the greedy action is selected?\n",
    "\n",
    "**A:** The probability of selecting the greedy action is $1-\\epsilon = 1-0.5 = 0.5$. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2\n",
    "**Q:** \n",
    "> Consider a k-armed bandit problem with $k=4$ actions, denoted 1,2,3, and 4. Consider applying to this problem a bandit algorithm using $\\epsilon$-greedy, sample-average action-value estimates, and initial estimates of $A_1(a)=0, \\forall a$. Suppose the initial sequence of actions and rewards is $A_1=1, R_1=-1,A_2=2,R_2=1,A_3=2,R_3=-2,A_4=2,R_4=2,A_5=3,R_5=0$. \n",
    "On some of these time steps the $\\epsilon$ case may have occurred, causing an action to be selected at random. On which time steps did this definitely occur? On which time steps could this possibly have occurred?\n",
    "\n",
    "**A:** The action-value estimate $Q_t(a)$ for each action $a$ after each time step $t$ is shown in the following table:\n",
    "\n",
    "| $t$ | $A_1$ | $A_2$ | $A_3$ | $A_4$ |\n",
    "|-----|-------|-------|-------|-------|\n",
    "|  1  |   -1  |   0   |   0   |   0   |\n",
    "|  2  |   -1  |   1   |   0   |   0   |\n",
    "|  3  |   -1  |  -0.5 |   0   |   0   |\n",
    "|  4  |   -1  | 0.333 |   0   |   0   |\n",
    "|  5  |   -1  | 0.333 |   0   |   0   |\n",
    "\n",
    "At time step $5$, $A_3$ is selected, despite $A_2$ having the highest action-value estimate of $0.333$, and so the $\\epsilon$ case must have occurred here (the greedy choice would be to choose $A_2$). The $\\epsilon$ case could also have occurred on time steps $1$ and $2$, however in these cases there were multiple actions with the maximum action-value estimate, and so the agent would have chosen one of these randomly even if the $\\epsilon$ case did not occur.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3\n",
    "\n",
    "**Q:**\n",
    "> In the comparison shown in Figure 2.2, which method will perform best in the long run in terms of cumulative reward and probability of selecting the best action? How much better will it be? Express your answer quantitatively.\n",
    "\n",
    "**A:** In this example, the optimum reward-per-step is 'about $1.55$', and the reward $R_t$ of selecting action $A_t$ is sampled from a normal distribution with mean $q_*(A_t)$ and variance 1.\n",
    "\n",
    "In the long run (once all methods have converged on the optimal action), the greedy method converged to an average reward-per-step of 1. The $\\epsilon=0.1$ method selects the optimal action with probability $0.9$ of the time, giving an average reward-per-step of $0.9 \\times 1.55 = 1.395$. The $\\epsilon=0.01$ selects the optimal action with a probability of $0.99$, giving an average reward-per-step of $0.99 \\times 1.55 = 1.5345$.\n",
    "\n",
    "In terms of cumulative reward after convergence, the $\\epsilon=0.01$ method will perform best. However in the example shown, up to the maximum 1000 steps, the $\\epsilon=0.1$ method performs best, and it is clear from visual inspection that this method has the greatest area under the curve (cumulative reward).\n",
    "\n",
    "In general, cumulative reward depends on how many steps the agents are given in the environment. Agents that are slower to learn (smaller $\\epsilon$ values), may perform better over very large numbers of steps. \n",
    "\n",
    "Theoretically the best possible performance is achieved if the number of steps $N \\to \\infty$ and $\\epsilon \\to 0$.\n",
    "\n",
    "After an arbitrarily high number of time steps, $\\epsilon=0.1$ selects the optimal action roughly $90\\%$ of the time, $\\epsilon=0.01$ selects the optimal action roughly $99\\%$ of the time.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.4\n",
    "**Q:**\n",
    "> If the step-size parameters, $\\alpha_n$, are not constant, then the estimate $Q_n$ is a weighted average of previously received rewards with a weighting different from that given by (2.6). What is the weighting on each prior reward for the general case, analogous to (2.6), in terms of the sequence of step-size parameters.\n",
    "\n",
    "**A:** The key difference from (2.6) is starting with $\\alpha_n$ instead of $\\alpha$:\n",
    "\n",
    "$Q_{n+1}=Q_n + \\alpha_n [ R_n - Q_n ]$\n",
    "\n",
    "$\\Rightarrow Q_{n+1}=\\alpha_n R_n + (1-\\alpha_n) Q_n$\n",
    "\n",
    "$\\Rightarrow Q_{n+1}=\\alpha_n R_n + (1-\\alpha_n)[\\alpha_{n-1}R_{n-1} + (1-\\alpha_{n-1})Q_{n-1}]$\n",
    "\n",
    "$\\displaystyle \\Rightarrow Q_{n+1}= \\alpha_n R_n + \\sum_{i=1}^{n-1}{\\left[ \\alpha_i \\prod_{j=i+1}^n{(1-\\alpha_j)} \\right]} + Q_1 \\prod_{i=1}^n{(1-\\alpha_n)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
